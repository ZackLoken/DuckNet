{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch.package.package_importer import PackageImporter\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "import os\n",
    "import modellib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Package DuckNet model</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "─── basemodel.pt.zip\n",
      "    ├── .data\n",
      "    │   ├── 0.storage\n",
      "    │   ├── 1.storage\n",
      "    │   ├── 10.storage\n",
      "    │   ├── 100.storage\n",
      "    │   ├── 101.storage\n",
      "    │   ├── 102.storage\n",
      "    │   ├── 103.storage\n",
      "    │   ├── 104.storage\n",
      "    │   ├── 105.storage\n",
      "    │   ├── 106.storage\n",
      "    │   ├── 107.storage\n",
      "    │   ├── 108.storage\n",
      "    │   ├── 109.storage\n",
      "    │   ├── 11.storage\n",
      "    │   ├── 110.storage\n",
      "    │   ├── 111.storage\n",
      "    │   ├── 112.storage\n",
      "    │   ├── 113.storage\n",
      "    │   ├── 114.storage\n",
      "    │   ├── 115.storage\n",
      "    │   ├── 116.storage\n",
      "    │   ├── 117.storage\n",
      "    │   ├── 118.storage\n",
      "    │   ├── 119.storage\n",
      "    │   ├── 12.storage\n",
      "    │   ├── 120.storage\n",
      "    │   ├── 121.storage\n",
      "    │   ├── 122.storage\n",
      "    │   ├── 123.storage\n",
      "    │   ├── 124.storage\n",
      "    │   ├── 125.storage\n",
      "    │   ├── 126.storage\n",
      "    │   ├── 127.storage\n",
      "    │   ├── 128.storage\n",
      "    │   ├── 129.storage\n",
      "    │   ├── 13.storage\n",
      "    │   ├── 130.storage\n",
      "    │   ├── 131.storage\n",
      "    │   ├── 132.storage\n",
      "    │   ├── 133.storage\n",
      "    │   ├── 134.storage\n",
      "    │   ├── 135.storage\n",
      "    │   ├── 136.storage\n",
      "    │   ├── 137.storage\n",
      "    │   ├── 138.storage\n",
      "    │   ├── 139.storage\n",
      "    │   ├── 14.storage\n",
      "    │   ├── 140.storage\n",
      "    │   ├── 141.storage\n",
      "    │   ├── 142.storage\n",
      "    │   ├── 143.storage\n",
      "    │   ├── 144.storage\n",
      "    │   ├── 145.storage\n",
      "    │   ├── 146.storage\n",
      "    │   ├── 147.storage\n",
      "    │   ├── 148.storage\n",
      "    │   ├── 149.storage\n",
      "    │   ├── 15.storage\n",
      "    │   ├── 150.storage\n",
      "    │   ├── 151.storage\n",
      "    │   ├── 152.storage\n",
      "    │   ├── 153.storage\n",
      "    │   ├── 154.storage\n",
      "    │   ├── 155.storage\n",
      "    │   ├── 156.storage\n",
      "    │   ├── 157.storage\n",
      "    │   ├── 158.storage\n",
      "    │   ├── 159.storage\n",
      "    │   ├── 16.storage\n",
      "    │   ├── 160.storage\n",
      "    │   ├── 161.storage\n",
      "    │   ├── 162.storage\n",
      "    │   ├── 163.storage\n",
      "    │   ├── 164.storage\n",
      "    │   ├── 165.storage\n",
      "    │   ├── 166.storage\n",
      "    │   ├── 167.storage\n",
      "    │   ├── 168.storage\n",
      "    │   ├── 169.storage\n",
      "    │   ├── 17.storage\n",
      "    │   ├── 170.storage\n",
      "    │   ├── 171.storage\n",
      "    │   ├── 172.storage\n",
      "    │   ├── 173.storage\n",
      "    │   ├── 174.storage\n",
      "    │   ├── 175.storage\n",
      "    │   ├── 176.storage\n",
      "    │   ├── 177.storage\n",
      "    │   ├── 178.storage\n",
      "    │   ├── 179.storage\n",
      "    │   ├── 18.storage\n",
      "    │   ├── 180.storage\n",
      "    │   ├── 181.storage\n",
      "    │   ├── 182.storage\n",
      "    │   ├── 183.storage\n",
      "    │   ├── 184.storage\n",
      "    │   ├── 185.storage\n",
      "    │   ├── 186.storage\n",
      "    │   ├── 187.storage\n",
      "    │   ├── 188.storage\n",
      "    │   ├── 189.storage\n",
      "    │   ├── 19.storage\n",
      "    │   ├── 190.storage\n",
      "    │   ├── 191.storage\n",
      "    │   ├── 192.storage\n",
      "    │   ├── 193.storage\n",
      "    │   ├── 194.storage\n",
      "    │   ├── 195.storage\n",
      "    │   ├── 196.storage\n",
      "    │   ├── 197.storage\n",
      "    │   ├── 198.storage\n",
      "    │   ├── 199.storage\n",
      "    │   ├── 2.storage\n",
      "    │   ├── 20.storage\n",
      "    │   ├── 200.storage\n",
      "    │   ├── 201.storage\n",
      "    │   ├── 202.storage\n",
      "    │   ├── 203.storage\n",
      "    │   ├── 204.storage\n",
      "    │   ├── 205.storage\n",
      "    │   ├── 206.storage\n",
      "    │   ├── 207.storage\n",
      "    │   ├── 208.storage\n",
      "    │   ├── 209.storage\n",
      "    │   ├── 21.storage\n",
      "    │   ├── 210.storage\n",
      "    │   ├── 211.storage\n",
      "    │   ├── 212.storage\n",
      "    │   ├── 213.storage\n",
      "    │   ├── 214.storage\n",
      "    │   ├── 215.storage\n",
      "    │   ├── 216.storage\n",
      "    │   ├── 217.storage\n",
      "    │   ├── 218.storage\n",
      "    │   ├── 219.storage\n",
      "    │   ├── 22.storage\n",
      "    │   ├── 220.storage\n",
      "    │   ├── 221.storage\n",
      "    │   ├── 222.storage\n",
      "    │   ├── 223.storage\n",
      "    │   ├── 224.storage\n",
      "    │   ├── 225.storage\n",
      "    │   ├── 226.storage\n",
      "    │   ├── 227.storage\n",
      "    │   ├── 228.storage\n",
      "    │   ├── 229.storage\n",
      "    │   ├── 23.storage\n",
      "    │   ├── 230.storage\n",
      "    │   ├── 231.storage\n",
      "    │   ├── 232.storage\n",
      "    │   ├── 233.storage\n",
      "    │   ├── 234.storage\n",
      "    │   ├── 235.storage\n",
      "    │   ├── 236.storage\n",
      "    │   ├── 237.storage\n",
      "    │   ├── 238.storage\n",
      "    │   ├── 239.storage\n",
      "    │   ├── 24.storage\n",
      "    │   ├── 240.storage\n",
      "    │   ├── 241.storage\n",
      "    │   ├── 242.storage\n",
      "    │   ├── 243.storage\n",
      "    │   ├── 244.storage\n",
      "    │   ├── 245.storage\n",
      "    │   ├── 246.storage\n",
      "    │   ├── 247.storage\n",
      "    │   ├── 248.storage\n",
      "    │   ├── 249.storage\n",
      "    │   ├── 25.storage\n",
      "    │   ├── 250.storage\n",
      "    │   ├── 251.storage\n",
      "    │   ├── 252.storage\n",
      "    │   ├── 253.storage\n",
      "    │   ├── 254.storage\n",
      "    │   ├── 255.storage\n",
      "    │   ├── 256.storage\n",
      "    │   ├── 257.storage\n",
      "    │   ├── 258.storage\n",
      "    │   ├── 259.storage\n",
      "    │   ├── 26.storage\n",
      "    │   ├── 260.storage\n",
      "    │   ├── 261.storage\n",
      "    │   ├── 262.storage\n",
      "    │   ├── 263.storage\n",
      "    │   ├── 264.storage\n",
      "    │   ├── 265.storage\n",
      "    │   ├── 266.storage\n",
      "    │   ├── 267.storage\n",
      "    │   ├── 268.storage\n",
      "    │   ├── 269.storage\n",
      "    │   ├── 27.storage\n",
      "    │   ├── 270.storage\n",
      "    │   ├── 271.storage\n",
      "    │   ├── 272.storage\n",
      "    │   ├── 273.storage\n",
      "    │   ├── 274.storage\n",
      "    │   ├── 275.storage\n",
      "    │   ├── 276.storage\n",
      "    │   ├── 277.storage\n",
      "    │   ├── 278.storage\n",
      "    │   ├── 279.storage\n",
      "    │   ├── 28.storage\n",
      "    │   ├── 280.storage\n",
      "    │   ├── 281.storage\n",
      "    │   ├── 282.storage\n",
      "    │   ├── 283.storage\n",
      "    │   ├── 284.storage\n",
      "    │   ├── 285.storage\n",
      "    │   ├── 286.storage\n",
      "    │   ├── 287.storage\n",
      "    │   ├── 288.storage\n",
      "    │   ├── 289.storage\n",
      "    │   ├── 29.storage\n",
      "    │   ├── 290.storage\n",
      "    │   ├── 291.storage\n",
      "    │   ├── 292.storage\n",
      "    │   ├── 293.storage\n",
      "    │   ├── 294.storage\n",
      "    │   ├── 295.storage\n",
      "    │   ├── 296.storage\n",
      "    │   ├── 297.storage\n",
      "    │   ├── 298.storage\n",
      "    │   ├── 299.storage\n",
      "    │   ├── 3.storage\n",
      "    │   ├── 30.storage\n",
      "    │   ├── 300.storage\n",
      "    │   ├── 301.storage\n",
      "    │   ├── 302.storage\n",
      "    │   ├── 303.storage\n",
      "    │   ├── 304.storage\n",
      "    │   ├── 305.storage\n",
      "    │   ├── 306.storage\n",
      "    │   ├── 307.storage\n",
      "    │   ├── 31.storage\n",
      "    │   ├── 32.storage\n",
      "    │   ├── 33.storage\n",
      "    │   ├── 34.storage\n",
      "    │   ├── 35.storage\n",
      "    │   ├── 36.storage\n",
      "    │   ├── 37.storage\n",
      "    │   ├── 38.storage\n",
      "    │   ├── 39.storage\n",
      "    │   ├── 4.storage\n",
      "    │   ├── 40.storage\n",
      "    │   ├── 41.storage\n",
      "    │   ├── 42.storage\n",
      "    │   ├── 43.storage\n",
      "    │   ├── 44.storage\n",
      "    │   ├── 45.storage\n",
      "    │   ├── 46.storage\n",
      "    │   ├── 47.storage\n",
      "    │   ├── 48.storage\n",
      "    │   ├── 49.storage\n",
      "    │   ├── 5.storage\n",
      "    │   ├── 50.storage\n",
      "    │   ├── 51.storage\n",
      "    │   ├── 52.storage\n",
      "    │   ├── 53.storage\n",
      "    │   ├── 54.storage\n",
      "    │   ├── 55.storage\n",
      "    │   ├── 56.storage\n",
      "    │   ├── 57.storage\n",
      "    │   ├── 58.storage\n",
      "    │   ├── 59.storage\n",
      "    │   ├── 6.storage\n",
      "    │   ├── 60.storage\n",
      "    │   ├── 61.storage\n",
      "    │   ├── 62.storage\n",
      "    │   ├── 63.storage\n",
      "    │   ├── 64.storage\n",
      "    │   ├── 65.storage\n",
      "    │   ├── 66.storage\n",
      "    │   ├── 67.storage\n",
      "    │   ├── 68.storage\n",
      "    │   ├── 69.storage\n",
      "    │   ├── 7.storage\n",
      "    │   ├── 70.storage\n",
      "    │   ├── 71.storage\n",
      "    │   ├── 72.storage\n",
      "    │   ├── 73.storage\n",
      "    │   ├── 74.storage\n",
      "    │   ├── 75.storage\n",
      "    │   ├── 76.storage\n",
      "    │   ├── 77.storage\n",
      "    │   ├── 78.storage\n",
      "    │   ├── 79.storage\n",
      "    │   ├── 8.storage\n",
      "    │   ├── 80.storage\n",
      "    │   ├── 81.storage\n",
      "    │   ├── 82.storage\n",
      "    │   ├── 83.storage\n",
      "    │   ├── 84.storage\n",
      "    │   ├── 85.storage\n",
      "    │   ├── 86.storage\n",
      "    │   ├── 87.storage\n",
      "    │   ├── 88.storage\n",
      "    │   ├── 89.storage\n",
      "    │   ├── 9.storage\n",
      "    │   ├── 90.storage\n",
      "    │   ├── 91.storage\n",
      "    │   ├── 92.storage\n",
      "    │   ├── 93.storage\n",
      "    │   ├── 94.storage\n",
      "    │   ├── 95.storage\n",
      "    │   ├── 96.storage\n",
      "    │   ├── 97.storage\n",
      "    │   ├── 98.storage\n",
      "    │   ├── 99.storage\n",
      "    │   ├── extern_modules\n",
      "    │   ├── python_version\n",
      "    │   ├── serialization_id\n",
      "    │   └── version\n",
      "    ├── model\n",
      "    │   ├── class_list.txt\n",
      "    │   └── model.pkl\n",
      "    ├── byteorder\n",
      "    ├── datasets.py\n",
      "    ├── modellib.py\n",
      "    └── traininglib.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = modellib.DuckDetector(classes_of_interest=[ # class order must match the label_dict from training\n",
    "                                    'AMCO', 'GADW', 'GWTE', 'Hen', 'MALL', 'NOPI', 'NSHO', 'REDH', 'RNDU'])\n",
    "model.save(\"basemodel.pt.zip\")\n",
    "imp = PackageImporter(\"basemodel.pt.zip\")\n",
    "print(imp.file_structure())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Open packaged model</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDetector(\n",
      "  (detector): Detector(\n",
      "    (basemodel): CustomRetinaNet(\n",
      "      (backbone): BackboneWithFPN(\n",
      "        (body): IntermediateLayerGetter(\n",
      "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "          (layer1): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "              (relu): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "              (relu): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (layer2): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "              (relu): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "              (relu): ReLU(inplace=True)\n",
      "            )\n",
      "            (3): Bottleneck(\n",
      "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "              (relu): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (layer3): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "              (relu): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "              (relu): ReLU(inplace=True)\n",
      "            )\n",
      "            (3): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "              (relu): ReLU(inplace=True)\n",
      "            )\n",
      "            (4): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "              (relu): ReLU(inplace=True)\n",
      "            )\n",
      "            (5): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "              (relu): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (layer4): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
      "              (relu): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
      "              (relu): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (fpn): FeaturePyramidNetwork(\n",
      "          (inner_blocks): ModuleList(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (2): Conv2dNormActivation(\n",
      "              (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (layer_blocks): ModuleList(\n",
      "            (0-3): 4 x Conv2dNormActivation(\n",
      "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (extra_blocks): LastLevelMaxPool()\n",
      "        )\n",
      "      )\n",
      "      (anchor_generator): AnchorGenerator()\n",
      "      (head): RetinaNetHead(\n",
      "        (classification_head): CustomRetinaNetClassificationHead(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (1): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (1): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2dNormActivation(\n",
      "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (1): ReLU(inplace=True)\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (1): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (cls_logits): Conv2d(256, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (dropout): Dropout(p=0.25, inplace=False)\n",
      "        )\n",
      "        (regression_head): CustomRetinaNetRegressionHead(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (1): ReLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (1): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2dNormActivation(\n",
      "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (1): ReLU(inplace=True)\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (1): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (bbox_reg): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (dropout): Dropout(p=0.25, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (transform): GeneralizedRCNNTransform(\n",
      "          Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
      "          Resize(min_size=(810,), max_size=1440, mode='bicubic')\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def load_model(file_path:str, model_name:str) -> \"torch.nn.Module\":\n",
    "            return PackageImporter(file_path).load_pickle(model_name, f'{model_name}.pkl', map_location='cpu')\n",
    "model = load_model(file_path='basemodel.pt.zip', model_name='model')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Process sample images </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn model.class_list into a dictionary\n",
    "print(f\"Model classes: {model.class_list}\")\n",
    "label_dict = {i+1: model.class_list[i] for i in range(len(model.class_list))}\n",
    "\n",
    "# distinct colors \n",
    "distinct_colors = ['#f032e6', '#ffffff', '#ffe119', '#3cb44b', '#42d4f4',\n",
    "                    '#f58231', '#e6194B', '#dcbeff', '#469990', '#4363d8']\n",
    "\n",
    "# label color map for plotting color-coded boxes by class\n",
    "label_color_map = {k: distinct_colors[i] for i, k in enumerate(label_dict.keys())}\n",
    "\n",
    "# classes are values in label_dict\n",
    "classes = list(label_dict.values())\n",
    "\n",
    "# reverse label dictionary for mapping predictions to classes\n",
    "rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "# distinct colors \n",
    "distinct_colors = ['#f032e6', '#ffffff', '#ffe119', '#3cb44b', '#42d4f4',\n",
    "                    '#f58231', '#e6194B', '#dcbeff', '#469990', '#4363d8']\n",
    "\n",
    "# label color map for plotting color-coded boxes by class\n",
    "label_color_map = {k: distinct_colors[i] for i, k in enumerate(label_dict.keys())}\n",
    "\n",
    "# function for reshaping boxes \n",
    "def get_box(boxes):\n",
    "    boxes = np.array(boxes)\n",
    "    boxes = boxes.astype('float').reshape(-1, 4)\n",
    "    if boxes.shape[0] == 1 : return boxes\n",
    "    return np.squeeze(boxes)\n",
    "\n",
    "# function for plotting image\n",
    "def img_show(image, ax = None, figsize = (6, 9)):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize = figsize)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.imshow(image)\n",
    "    return ax\n",
    " \n",
    "def plot_bbox(ax, boxes, labels):\n",
    "    # Determine if we have a string or tensor label\n",
    "    if isinstance(labels, str):\n",
    "        # String label - convert to numeric using rev_label_dict\n",
    "        numeric_label = rev_label_dict.get(labels, None)\n",
    "        color = label_color_map[numeric_label] if numeric_label in label_color_map else 'black'\n",
    "        display_text = labels  # Use the string label directly\n",
    "    else:\n",
    "        # Tensor/numeric label\n",
    "        numeric_label = labels.item() if hasattr(labels, 'item') else labels\n",
    "        color = label_color_map[numeric_label] if numeric_label in label_color_map else 'black'\n",
    "        display_text = label_dict[numeric_label] if numeric_label in label_dict else numeric_label\n",
    "    \n",
    "    # Draw bounding box\n",
    "    ax.add_patch(Rectangle((boxes[:, 0], boxes[:, 1]), \n",
    "                               boxes[:, 2] - boxes[:, 0], boxes[:, 3] - boxes[:, 1],\n",
    "                               fill=False,\n",
    "                               color=color, \n",
    "                               linewidth=1.25))\n",
    "    \n",
    "    # Add label text\n",
    "    ax.text(boxes[:, 2], boxes[:, 3], \n",
    "            display_text,\n",
    "            fontsize=8,\n",
    "            bbox=dict(facecolor='white', alpha=0.8, pad=0, edgecolor='none'),\n",
    "            color='black')\n",
    "\n",
    "# function for plotting all boxes and labels on the image using get_polygon, img_show, and plot_mask functions\n",
    "def plot_detections(image, boxes, labels, ax = None):\n",
    "    ax = img_show(image.permute(1, 2, 0), ax = ax)\n",
    "    for i in range(len(boxes)):\n",
    "        box = get_box(boxes[i])\n",
    "        plot_bbox(ax, box, labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'S:/Savanna Institute/Deep Learning/DuckNet/RetinaNet/filtered_images/DJI_20221216103105_0073_Z.JPG'\n",
    "prediction = model.process_image(image_path)\n",
    "print()\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print()\n",
    "image = model.load_image(image_path)\n",
    "image = to_tensor(image)\n",
    "\n",
    "plot_detections(image, prediction['boxes'], prediction['labels']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Plot sample batch of transformed data </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets module from basemodel.pt.zip\n",
    "file_path = 'basemodel.pt.zip'\n",
    "module = 'datasets'\n",
    "\n",
    "datasets = PackageImporter(file_path).import_module(module)\n",
    "\n",
    "def get_files(directory: str, extension: str):\n",
    "    \"\"\"\n",
    "    Returns a sorted list of full file paths from a directory that match the given extension.\n",
    "    Extension should include the dot (e.g. \".jpg\", \".json\").\n",
    "    \"\"\"\n",
    "    return [os.path.join(directory, f) for f in sorted(os.listdir(directory))\n",
    "            if f.lower().endswith(extension.lower())]\n",
    "\n",
    "# Create sample dataset\n",
    "jpg_dir = 'S:/Savanna Institute/Deep Learning/DuckNet/RetinaNet/filtered_images/'\n",
    "json_dir = 'S:/Savanna Institute/Deep Learning/DuckNet/RetinaNet/filtered_annotations/'\n",
    "\n",
    "jpgfiles = get_files(jpg_dir, '.jpg')\n",
    "jsonfiles = get_files(json_dir, '.json')\n",
    "\n",
    "sample_dataset = datasets.DetectionDataset( # type: ignore\n",
    "    jpgfiles=jpgfiles,\n",
    "    jsonfiles=jsonfiles,\n",
    "    augment=True,\n",
    "    negative_classes=[],\n",
    "    class_list=model.class_list\n",
    ")\n",
    "\n",
    "image, target = sample_dataset[2]\n",
    "print(image.shape)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataloader = datasets.create_dataloader(sample_dataset, batch_size = 8, shuffle = True, num_workers=0) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(sample_dataloader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the all samples from batch in a grid of subplots. \n",
    "plt.figure(figsize = (12, 32))\n",
    "for i in range(8):\n",
    "    ax = plt.subplot(8, 2, 1 + i)\n",
    "    plot_detections(images[i], targets[i]['boxes'], targets[i]['labels'], ax = ax)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Sample {i + 1}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Train model</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(file_path='basemodel.pt.zip', model_name='model')\n",
    "imagefiles_train = get_files('S:/Savanna Institute/Deep Learning/DuckNet/RetinaNet/test/images/', '.jpg')\n",
    "jsonfiles_train = get_files('S:/Savanna Institute/Deep Learning/DuckNet/RetinaNet/test/annotations/', '.json')\n",
    "imagefiles_test = get_files('S:/Savanna Institute/Deep Learning/DuckNet/RetinaNet/test/images/', '.jpg')\n",
    "jsonfiles_test = get_files('S:/Savanna Institute/Deep Learning/DuckNet/RetinaNet/test/annotations/', '.json')\n",
    "model.start_training_detector(imagefiles_train = imagefiles_train,\n",
    "                              jsonfiles_train = jsonfiles_train,\n",
    "                              imagefiles_test = imagefiles_test,\n",
    "                              jsonfiles_test = jsonfiles_test,\n",
    "                              negative_classes = [],\n",
    "                              epochs = 2,\n",
    "                              lr = 0.001,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Save finetuned model</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('basemodel_finetuned.pt.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Converting Darwin JSON to LabelMe JSON </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json \n",
    "\n",
    "# def get_imagename_from_jsonfile(jsonfile):\n",
    "#     with open(jsonfile, 'r') as j:\n",
    "#         jsondata = json.loads(j.read())\n",
    "#     return jsondata['item']['slots'][0]['source_files'][0]['file_name']\n",
    "\n",
    "\n",
    "# def get_boxes_from_jsonfile(jsonfile):\n",
    "#     '''Reads bounding boxes from a DARWIN json file and returns them as a (Nx4) array'''\n",
    "#     with open(jsonfile, 'r') as j:\n",
    "#         jsondata = json.loads(j.read())\n",
    "        \n",
    "#     boxes = []\n",
    "#     for i in range(len(jsondata['annotations'])):\n",
    "#         box = [[jsondata['annotations'][i]['bounding_box']['x'], # xmin\n",
    "#                 jsondata['annotations'][i]['bounding_box']['y']], # ymin\n",
    "#                 [jsondata['annotations'][i]['bounding_box']['x']+jsondata['annotations'][i]['bounding_box']['w'], # xmax\n",
    "#                 jsondata['annotations'][i]['bounding_box']['y']+jsondata['annotations'][i]['bounding_box']['h']]] # ymax\n",
    "#         boxes.append(box)\n",
    "#     return boxes # return as (Nx4) array of bounding\n",
    "\n",
    "\n",
    "# def get_labels_from_jsonfile(jsonfile):\n",
    "#     '''Reads a list of labels in a DARWIN json file.'''\n",
    "#     with open(jsonfile, 'r') as j:\n",
    "#         jsondata = json.loads(j.read())\n",
    "#     return [ a['name'] for a in jsondata['annotations'] ]\n",
    " \n",
    "\n",
    "# def get_imagesize_from_jsonfile(jsonfile):\n",
    "#     with open(jsonfile, 'r') as j:\n",
    "#         jsondata = json.loads(j.read())\n",
    "#     return (jsondata['item']['slots'][0]['height'], jsondata['item']['slots'][0]['width'])\n",
    "\n",
    "\n",
    "# def darwin_to_labelme_json(jsondata):\n",
    "#     # convert darwin json to labelme json format. \n",
    "\n",
    "#     # labelme json should have following format:\n",
    "#     # {'version': '4.5.6',\n",
    "#     #  'flags': {},\n",
    "#     #  'shapes': [\n",
    "#     #      {\n",
    "#     #       'label': 'duck',\n",
    "#     #       'points': [[xmin, ymin], [xmax, ymax]],\n",
    "#     #       'group_id': null,\n",
    "#     #       'shape_type': 'rectangle',\n",
    "#     #       'flags': {}\n",
    "#     #      },\n",
    "#     #      ...\n",
    "#     #  ],\n",
    "#     # 'imagePath': 'path/to/image/file',\n",
    "#     # 'imageData': 'base64 encoded image data',\n",
    "#     # 'imageHeight': 480,\n",
    "#     # 'imageWidth': 640}\n",
    "\n",
    "#     # ignore the 'imageData' field. \n",
    "\n",
    "#     image_name = get_imagename_from_jsonfile(jsondata)\n",
    "#     boxes = get_boxes_from_jsonfile(jsondata)\n",
    "#     labels = get_labels_from_jsonfile(jsondata)\n",
    "#     image_size = get_imagesize_from_jsonfile(jsondata)\n",
    "\n",
    "#     shapes = []\n",
    "#     for i in range(len(labels)):\n",
    "#         shape = {'label': labels[i],\n",
    "#                     'points': boxes[i],\n",
    "#                     'group_id': 'null',\n",
    "#                     'shape_type': 'rectangle',\n",
    "#                     'flags': {}}\n",
    "#         shapes.append(shape)\n",
    "\n",
    "#     for i in range(len(labels)):\n",
    "#         labelme_json = {'version': '4.5.6',\n",
    "#                         'flags': {},\n",
    "#                         'shapes': shapes,\n",
    "#                         'imagePath': image_name,\n",
    "#                         'imageData': '',\n",
    "#                         'imageHeight': image_size[0],\n",
    "#                         'imageWidth': image_size[1]}\n",
    "#     return labelme_json\n",
    "\n",
    "# jsonfile = \"S:/Savanna Institute/Deep Learning/DuckNet/Annotations/DJI_20221221103938_0079_Z.json\"\n",
    "# labelme_json = darwin_to_labelme_json(jsonfile)\n",
    "\n",
    "\n",
    "# with open('S:/Savanna Institute/Deep Learning/DuckNet/RetinaNet/filtered_annotations/DJI_20221221103938_0079_Z.json', 'w') as j:\n",
    "#         json.dump(labelme_json, j)\n",
    "\n",
    "# print(labelme_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# dir = 'C:/Users/zack/Desktop/DuckNet_Data/'\n",
    "\n",
    "# # create two new folders in dir: LabelMe_Annotations_Test and LabelMe_Annotations_Train\n",
    "# os.makedirs(dir + 'LabelMe_Annotations_Test', exist_ok = True)\n",
    "# os.makedirs(dir + 'LabelMe_Annotations_Train', exist_ok = True)\n",
    "\n",
    "# # convert all json files in Annotations_Test to labelme json format and save them in LabelMe_Annotations_Test\n",
    "# for jsonfile in os.listdir(dir + 'Annotations_Test/'):\n",
    "#     labelme_json = darwin_to_labelme_json(dir + 'Annotations_Test/' + jsonfile)\n",
    "#     with open(dir + 'LabelMe_Annotations_Test/' + jsonfile, 'w') as j:\n",
    "#         json.dump(labelme_json, j)\n",
    "\n",
    "# # convert all json files in Annotations_Train to labelme json format and save them in LabelMe_Annotations_Train\n",
    "# for jsonfile in os.listdir(dir + 'Annotations_Train/'):\n",
    "#     labelme_json = darwin_to_labelme_json(dir + 'Annotations_Train/' + jsonfile)\n",
    "#     with open(dir + 'LabelMe_Annotations_Train/' + jsonfile, 'w') as j:\n",
    "#         json.dump(labelme_json, j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bohb_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
